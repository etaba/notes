I like to think of my bugs as features or easter eggs. Just kidding. 

In my current job there was luckily only one time a bug seriously affected the production environment of my project. The project was an inventory replenishment feature and the bug was basically that users weren't being prevented from assigning a conversion factor of 0 for their items' units of measure, and if they accidentally did, they stopped receiving orders of the item. This affected hundreds of users and I received a call on the weekend. Dealing with the bug took place in 4 steps:

Step 1 - What bug?: First step is identifying the issue. Working with the on-site implementer (who filed the bug), we obtained secured tenanted access to the user's system through a VM to inspect and log the errant behavior. Debugging the robust logs revealed the problem. We found the functions responsible for kicking off the orders and inspected the criteria under which the won't do so. One of them was checking that the inventory on hand is less than the inventory needed. With a conversion factor of 0, naturally this is validation will always be false.

Step 2 - The Bandaid: The next step was to stop the bleeding. We communicated to the users the nature of the bug, and that any orders that can't wait can be fixed by adjusting the conversion factor, and also that they could expect a hands-free fix that day or the next. That same weekend I pushed a hot-patch that would intercept order requests, check if they would be affected by the bug, and if so perform some custom processing (read: hack) which basically assumed the conversion factor was in fact 1 not 0, allowing the order to proceed. This is the most critical (and stressful) step - getting the users back on their feet without breaking things further. 

Step 3 -  An Actual Solution: That week the next order of action was providing a permanent fix. Product owners were called in because a functional decision was needed: What exactly is the bug? The ability to enter a conversion factor of 0? Or the ability to order an item that has a conversion factor of 0? It was a functional question that we engineers get to weigh in on but ultimately it was the product owners who make the final call. Decision in hand we implement a permanent fix, remove the monkey patch, and push it through on the regular deployment cycle.

Step 4 - RCA: My company does a good job of not making Root Cause Analysis meetings awkward, as I've known them to be at past companies. The managers assure us the goal isn't to point fingers and instead we come together and talk about how these types of bugs arise and how they can be avoided. In this case, the root cause was poor cross-team integration. One team manages the conversion factors, another (our) team manages the ordering feature. The problem was our team wasn't aware of the complete specification of the other team's piece, and so a dangerous assumption was made. We agree to meet on a more regular basis to sync up our interrelated features and use the rest of the meeting to think about other areas that might need similar inspection.


At my current position we take Agile development pretty seriously. I'm the scrum master of a 9 person scrum team. We are fully cross functional, in that our team has a product manager, QA engineer, automation engineer, and regular devs. We operate on a 2 week sprint cycle and perform all of the agile event, which as scrum master I facilitate.

Daily Stand Up: We go around the room and discuss our updates: What have I done since yesterday, what am I working on right now, what will I finish before tomorrow, do I have any impediments. This meeting is to be no longer than 10-15 minutes, as ceremoniously symbolized by the fact that we're all standing up. We have a sprint board, which tracks the progress of each person's projects and we update it at this time. As scrum master I keep an eye out for red flags like updates about irrelevant tasks, impediments that aren't in progress of being resolved, or projects that aren't on track to be finished within the sprint. 

Backlog Refinement: The product owner/manager drives this meeting, where we go through our product backlog to break down features and projects into smaller tasks that can fit into a sprint cycle. The tasks are organized and prioritized. These meetings occur twice per sprint.

Sprint Planning: At the beginning of the sprint we plan by examining the backlog and pulling tasks from it to fill our sprint backlog. This is the work we're committing to deliver in the sprint and it's at this time that devs agree on which tasks they'll be working on personally.

Sprint Review: At the end of the sprint we review the work delivered. All stakeholders are invited and I present a sprint burn-down chart which tracks the completion of the work throughout the sprint as well as a burn-up chart which shows how we're trending for the 6 month release. 

Retrospective: The retrospective is an opportunity to reflect on what the team did well and what we could improve. Everyone has an opportunity to speak up and we agree on several action items we want to implement in the next sprint. The retrospective is how we continuously improve the team's process and happiness.

We do have remote developers, and I myself am remote for most the week. To accommodate, we use online tools including jira, cardboardit, trello, zoom, and slack. We make it a standard to have your camera on in the meeting because we found that this helps bridge the distance and facilitates the ability to interject and be understood over the network. 


I'd like to contribute to the BetterTouchTool project. (https://github.com/folivoraAI/BetterTouchTool) It's a program I've used as long as I've had a mac which adds a bunch of useful UX features like window management and hot keys. I'd like to work on the TouchBar feature because while what they've done so far has been game changing especially considering Apple has completely shunned this feature, there's so much more it could do. I've actually already done some work to add some features to this project with my own better touch tool plugin (https://github.com/etaba/TickerBar) but if I had enough time I'd love to make more integrated changes to the actual codebase. I'd start by forking the repository, learning how everything works together and identifying the piece responsible for the TouchBar features. Then I'd make my changes and add the ability to create dynamic menu's (currently you can only either create static menus or dynamic outputs but there's no way to combine the two). After updating the test suites and rebasing, I'd make a pull request and be polite and responsive to the code reviews I receive.

My current and last few employers all have followed a TDD approach, which I find is best. The tests come first and reinforce the acceptance criteria of the project. As the project is developed, the tests begin to pass, some tests are changed, and some new tests are added as the requirements evolve.  

One example was an inventory item set-up feature. We made it 3 years ago and it's supported by many unit tests which test small units of code but also system tests which integrate the feature into full product flows. Since the tests were made they've proven their worth time and time again. Before we push changes to production the tests validate the operability of the code. When other teams push features which affect ours in some way, the automation fails safely and the proper adjustments can be made. And now 3 years later we have been tasked with refactoring the whole thing, the tests are the most efficient tool we have to make sure our changes are backwards compatible.

1. You mentioned the value of a comprehensive test suite when refactoring. What are some of the tools and architectural patterns you have found to be the most valueable when writing tests?

2. What would you say was your best experience of version control workflows in a team? Which parts of it did you find particularly helpful? Were there any pain-points that you are still searching for ways to resolve?

3. Can you think of an example where a decision you made introduced more technical debt than you anticipated? How has this influenced your decision-making process since then?

4. Can you think of any situations where you and another team member had different opinions on how to solve a bug or implement a feature? What process was followed to come to a final decision?




1. My company uses its own proprietary testing suite which is why I omitted mentioning it by name in the previous response. With it we employ a TDD approach which reinforces the acceptance criteria and confirmations of the code we write. The tests are written first and the code doesn't meet the "definition of ready" until the tests are all passing. There are two kinds of tests we implement with the tooling; Unit tests which test single methods or small segments of code, and system tests which chain together functional flows of tests to make sure all the different pieces of a project are working well after being integrated. 

2. My best experience of version control has been at my current employer. Rather than forking separate branches, changes are wrapped by functional toggles. The toggles are given a confidence level which determine which tenants are exposed to which code. At the end of each release cycle the toggles are advanced to the production confidence level and the new changes become available to the customers. This allows us to easily differentiate between code which needs to target production quickly (for example, a bug fix or hot patch) vs the code we want to release to specific customers in a preview window, vs the code that remains internal and available only to developers until the end of the release cycle. I still favor traditional versioning systems like Git for smaller projects, but with a codeline as immense as Workday's we're lucky to have such a versatile and seamless alternative.

3. Once during a hackathon I felt the need to swap out our SQLite backend with a MySQL one. Sure, as the project scaled this would be a good idea and the sooner the change was made the easier it would be. But in the environment we were in (a 24 hour window), the project was clearly not going to scale past the limits of SQLite. I think back to that hackathon as a microcosm of production development. It's important to look at all the factors surrounding the project rather than just focusing on what might be the perfect solution from a single viewpoint (in this case, database efficiency). What I should've done was realized that development time was our most important resource, not how fast our database could service large amounts of queries. I spent an hour refactoring the db backend and had I used that hour elsewhere we might've been able to actually deliver a functioning product within the time limit. 

4. My coworker and I once disagreed about the need to index a particular field of a data model. I didn't think the extra memory needed for the table was worth the negligible look-up performance increase (the field could only have a handful of values).  We didn't have time to spike each way and benchmark the results so we grabbed a room and talked each other through our thinking with the whiteboard. I did my research and we calculated the performance difference using realistic benchmarks from the internet. We worked through the scenarios together so that when the answer was right there on the board at the end of the meeting it felt like both of us had come to the realization together.